Couldn't find '/home/hfc/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKeum9Cl7qVSxqzQzrP4mY9ntvMMQ4K/gZiz3cAgo7qp

2025/04/04 18:56:28 routes.go:1231: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/hfc/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-04-04T18:56:28.340+08:00 level=INFO source=images.go:458 msg="total blobs: 0"
time=2025-04-04T18:56:28.340+08:00 level=INFO source=images.go:465 msg="total unused blobs removed: 0"
time=2025-04-04T18:56:28.341+08:00 level=INFO source=routes.go:1298 msg="Listening on 127.0.0.1:11434 (version 0.6.4)"
time=2025-04-04T18:56:28.342+08:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="433.7 MiB"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-a12a891f-f7a9-80bb-b7be-75023d0b09f2 library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="125.7 MiB"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-65c85600-e089-d091-edab-738b9e487c22 library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="101.7 MiB"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-7216d8b7-5bd2-ce19-3833-a353d4261825 library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="2.1 GiB"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-09c0641d-f776-bbe5-6211-31b61fd2ef8b library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="197.7 MiB"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-141da65b-4c7a-fdf4-8f23-cea937e4d38c library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="117.7 MiB"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="23.5 GiB"
time=2025-04-04T18:56:30.132+08:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-55c12161-e849-4ab9-1f7b-9470821e7df4 library=cuda variant=v11 compute=8.6 driver=11.4 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="23.5 GiB"
[GIN] 2025/04/05 - 15:16:41 | 200 |   14.442993ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 15:16:41 | 404 |     338.344µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/05 - 15:17:15 | 200 |      73.177µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 15:17:15 | 404 |     154.775µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/05 - 15:17:34 | 200 |      44.874µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 15:17:34 | 404 |     148.102µs |       127.0.0.1 | POST     "/api/show"
time=2025-04-05T15:17:36.025+08:00 level=INFO source=download.go:177 msg="downloading 667b0c1932bc in 16 307 MB part(s)"
time=2025-04-05T16:46:02.421+08:00 level=INFO source=download.go:177 msg="downloading 948af2743fc7 in 1 1.5 KB part(s)"
time=2025-04-05T16:46:04.180+08:00 level=INFO source=download.go:177 msg="downloading 0ba8f0e314b4 in 1 12 KB part(s)"
time=2025-04-05T16:46:05.843+08:00 level=INFO source=download.go:177 msg="downloading 56bb8bd477a5 in 1 96 B part(s)"
time=2025-04-05T16:46:07.502+08:00 level=INFO source=download.go:177 msg="downloading 455f34728c9b in 1 487 B part(s)"
[GIN] 2025/04/05 - 16:46:21 | 200 |      1h28m47s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/04/05 - 16:50:37 | 200 |  377.714781ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-05T16:50:38.871+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T16:50:38.872+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-05T16:50:38.872+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-05T16:50:38.872+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-05T16:50:40.190+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="191.2 GiB" free_swap="0 B"
time=2025-04-05T16:50:40.190+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T16:50:40.190+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-05T16:50:40.190+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-05T16:50:40.190+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T16:50:40.551+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 42677"
time=2025-04-05T16:50:40.552+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T16:50:40.552+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T16:50:40.552+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T16:50:40.603+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T16:50:41.117+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T16:50:41.119+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:42677"
time=2025-04-05T16:50:41.307+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T16:50:48.161+08:00 level=INFO source=server.go:619 msg="llama runner started in 7.61 seconds"
[GIN] 2025/04/05 - 16:50:48 | 200 |  10.69345508s |       127.0.0.1 | POST     "/api/generate"
time=2025-04-05T18:53:53.317+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T18:53:53.317+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-05T18:53:53.317+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-05T18:53:53.318+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-05T18:53:54.561+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="190.9 GiB" free_swap="0 B"
time=2025-04-05T18:53:54.561+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T18:53:54.561+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-05T18:53:54.561+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-05T18:53:54.561+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T18:53:54.885+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 35417"
time=2025-04-05T18:53:54.885+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T18:53:54.885+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T18:53:54.885+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T18:53:54.906+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T18:53:54.980+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T18:53:54.980+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:35417"
time=2025-04-05T18:53:55.137+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T18:53:57.399+08:00 level=INFO source=server.go:619 msg="llama runner started in 2.51 seconds"
[GIN] 2025/04/05 - 18:54:02 | 200 | 10.207197237s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 18:54:53 | 200 |      57.083µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 18:54:53 | 200 |   41.605859ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/05 - 18:55:32 | 200 |      61.539µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 18:55:32 | 200 |   63.157844ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/05 - 19:00:24 | 200 |      58.509µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:02:35 | 201 | 55.220553439s |       127.0.0.1 | POST     "/api/blobs/sha256:8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e"
[GIN] 2025/04/05 - 19:02:35 | 200 |  241.746698ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/04/05 - 19:06:16 | 200 |       54.79µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:06:16 | 200 |    75.72596ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-05T19:06:18.240+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:06:18.240+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-05T19:06:19.713+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="190.8 GiB" free_swap="0 B"
time=2025-04-05T19:06:19.713+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:06:19.728+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T19:06:20.012+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 33493"
time=2025-04-05T19:06:20.013+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T19:06:20.013+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T19:06:20.013+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T19:06:20.038+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T19:06:20.266+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T19:06:20.266+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:33493"
time=2025-04-05T19:06:20.516+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T19:06:25.793+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.78 seconds"
[GIN] 2025/04/05 - 19:06:25 | 200 |  9.071535921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:07:04 | 200 |  4.801013843s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:07:21 | 200 |  968.673211ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:07:37 | 200 |  1.538895716s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:07:56 | 200 |   709.63709ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:08:10 | 200 |  3.126164984s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:08:25 | 200 |  1.235094202s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:08:31 | 200 |  1.219962192s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:08:52 | 200 |  6.509363185s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:09:11 | 200 | 10.271658816s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:10:12 | 200 | 45.860399017s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:10:37 | 200 |      85.749µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:10:37 | 200 |   66.896993ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 library=cuda total="23.7 GiB" available="2.1 GiB"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-a12a891f-f7a9-80bb-b7be-75023d0b09f2 library=cuda total="23.7 GiB" available="1.1 GiB"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-65c85600-e089-d091-edab-738b9e487c22 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-7216d8b7-5bd2-ce19-3833-a353d4261825 library=cuda total="23.7 GiB" available="841.7 MiB"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-09c0641d-f776-bbe5-6211-31b61fd2ef8b library=cuda total="23.7 GiB" available="485.7 MiB"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-141da65b-4c7a-fdf4-8f23-cea937e4d38c library=cuda total="23.7 GiB" available="501.7 MiB"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d library=cuda total="23.7 GiB" available="7.0 GiB"
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-55c12161-e849-4ab9-1f7b-9470821e7df4 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-05T19:10:39.449+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:10:39.449+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-05T19:10:39.449+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-05T19:10:39.449+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-55c12161-e849-4ab9-1f7b-9470821e7df4 parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-05T19:10:40.721+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="188.7 GiB" free_swap="0 B"
time=2025-04-05T19:10:40.721+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:10:40.721+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-05T19:10:40.721+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-05T19:10:40.721+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T19:10:41.000+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 38677"
time=2025-04-05T19:10:41.001+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=2
time=2025-04-05T19:10:41.001+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T19:10:41.001+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T19:10:41.023+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T19:10:41.119+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T19:10:41.119+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:38677"
time=2025-04-05T19:10:41.253+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T19:10:43.513+08:00 level=INFO source=server.go:619 msg="llama runner started in 2.51 seconds"
[GIN] 2025/04/05 - 19:10:43 | 200 |  5.519201461s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:10:52 | 200 |  982.756087ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:10:57 | 200 |  320.890643ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:11:05 | 200 |      27.021µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:11:05 | 200 |   58.923845ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/05 - 19:11:05 | 200 |   41.400044ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:11:18 | 200 |   6.66194434s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/05 - 19:13:29 | 200 |     457.442µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:13:29 | 200 |     235.871µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/04/05 - 19:14:12 | 200 |      30.937µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:14:12 | 200 |    1.591172ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:14:13 | 200 |      74.815µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:14:13 | 200 |      55.006µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/04/05 - 19:17:40 | 200 |      65.148µs |       127.0.0.1 | GET      "/"
[GIN] 2025/04/05 - 19:17:40 | 404 |      10.552µs |       127.0.0.1 | GET      "/favicon.ico"
[GIN] 2025/04/05 - 19:21:15 | 400 |     122.369µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:21:47 | 400 |     221.462µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:22:23 | 400 |     121.243µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:22:49 | 400 |      107.92µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:25:01 | 200 |      67.171µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:25:01 | 200 |   59.903833ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-05T19:25:03.336+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:25:03.336+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-05T19:25:04.675+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="190.5 GiB" free_swap="0 B"
time=2025-04-05T19:25:04.675+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:25:04.676+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T19:25:04.952+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 42841"
time=2025-04-05T19:25:04.953+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T19:25:04.953+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T19:25:04.953+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T19:25:04.979+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T19:25:05.086+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T19:25:05.086+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:42841"
time=2025-04-05T19:25:05.205+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T19:25:10.229+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.28 seconds"
[GIN] 2025/04/05 - 19:25:10 | 200 |  8.393701623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:26:32 | 404 |       9.855µs |       127.0.0.1 | GET      "/apple-touch-icon-precomposed.png"
[GIN] 2025/04/05 - 19:26:32 | 404 |       6.298µs |       127.0.0.1 | GET      "/apple-touch-icon.png"
[GIN] 2025/04/05 - 19:26:32 | 404 |       6.852µs |       127.0.0.1 | GET      "/favicon.ico"
[GIN] 2025/04/05 - 19:26:32 | 404 |       9.784µs |       127.0.0.1 | GET      "/api/generate"
[GIN] 2025/04/05 - 19:26:52 | 400 |       71.94µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:29:56 | 200 |      58.627µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 19:29:56 | 200 |      42.654µs |       127.0.0.1 | GET      "/api/ps"
time=2025-04-05T19:30:58.288+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:30:58.289+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-05T19:30:59.627+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="190.5 GiB" free_swap="0 B"
time=2025-04-05T19:30:59.627+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T19:30:59.628+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T19:30:59.987+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 40693"
time=2025-04-05T19:30:59.988+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T19:30:59.988+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T19:30:59.988+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T19:31:00.015+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T19:31:00.102+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T19:31:00.102+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:40693"
time=2025-04-05T19:31:00.240+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T19:31:05.263+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.27 seconds"
[GIN] 2025/04/05 - 19:31:06 | 200 |  9.805587016s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 19:34:09 | 200 |  161.666156ms |       127.0.0.1 | POST     "/api/generate"
time=2025-04-05T21:44:58.204+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T21:44:58.204+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-05T21:44:59.479+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="187.4 GiB" free_swap="0 B"
time=2025-04-05T21:44:59.479+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T21:44:59.479+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T21:44:59.765+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 37525"
time=2025-04-05T21:44:59.765+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T21:44:59.765+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T21:44:59.766+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T21:44:59.791+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T21:44:59.902+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T21:44:59.902+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:37525"
time=2025-04-05T21:45:00.018+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T21:45:05.042+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.28 seconds"
[GIN] 2025/04/05 - 21:45:06 | 200 |  9.282382751s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 21:46:32 | 200 |  103.462326ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 21:47:43 | 200 |  114.756694ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 21:47:56 | 200 |   90.428906ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 21:53:42 | 200 |      56.532µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 21:53:42 | 200 |      45.422µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/04/05 - 21:53:49 | 200 |      27.121µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/05 - 21:53:49 | 200 |   68.881747ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-05T21:53:51.032+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T21:53:51.032+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-05T21:53:52.381+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="186.1 GiB" free_swap="0 B"
time=2025-04-05T21:53:52.381+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T21:53:52.381+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T21:53:52.671+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 38361"
time=2025-04-05T21:53:52.672+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T21:53:52.672+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T21:53:52.672+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T21:53:52.698+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T21:53:52.818+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T21:53:52.818+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:38361"
time=2025-04-05T21:53:52.924+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T21:53:57.697+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.02 seconds"
[GIN] 2025/04/05 - 21:53:57 | 200 |   8.20278368s |       127.0.0.1 | POST     "/api/generate"
time=2025-04-05T22:07:39.224+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T22:07:40.327+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-05T22:07:41.677+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="165.2 GiB" free_swap="0 B"
time=2025-04-05T22:07:41.677+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T22:07:41.677+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T22:07:42.077+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 43471"
time=2025-04-05T22:07:42.077+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T22:07:42.077+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T22:07:42.077+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T22:07:42.093+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T22:07:42.218+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T22:07:42.218+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:43471"
time=2025-04-05T22:07:42.329+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T22:07:47.349+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.27 seconds"
[GIN] 2025/04/05 - 22:07:48 | 200 | 11.091848301s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-05T22:21:11.035+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T22:21:11.036+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-05T22:21:12.427+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="183.6 GiB" free_swap="0 B"
time=2025-04-05T22:21:12.427+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-05T22:21:12.427+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-05T22:21:12.720+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 43143"
time=2025-04-05T22:21:12.721+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-05T22:21:12.721+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-05T22:21:12.721+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-05T22:21:12.746+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-05T22:21:12.834+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-05T22:21:12.834+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:43143"
time=2025-04-05T22:21:12.973+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-8459bc96d58abddad1409219a657f494f4bd5aa5bf49096aa0ad198adc110e4e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-05T22:21:17.998+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.28 seconds"
[GIN] 2025/04/05 - 22:21:19 | 200 | 10.211889738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 22:22:50 | 200 |  1.102849362s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 22:25:26 | 200 |  1.613330752s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/05 - 22:25:28 | 200 |  1.687992925s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/08 - 22:40:37 | 200 |    1.298027ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/08 - 22:40:37 | 200 |     334.925µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/04/08 - 22:40:41 | 200 |      58.551µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/08 - 22:40:41 | 200 |  152.594667ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-08T22:40:43.415+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-08T22:40:43.437+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-08T22:40:43.437+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-08T22:40:43.438+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-08T22:40:44.707+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="190.2 GiB" free_swap="0 B"
time=2025-04-08T22:40:44.708+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-08T22:40:44.708+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-08T22:40:44.708+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-08T22:40:44.708+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-08T22:40:45.035+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 39277"
time=2025-04-08T22:40:45.035+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-08T22:40:45.035+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-08T22:40:45.036+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-08T22:40:45.062+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-08T22:40:45.577+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-08T22:40:45.578+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:39277"
time=2025-04-08T22:40:45.792+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-08T22:41:33.765+08:00 level=INFO source=server.go:619 msg="llama runner started in 48.73 seconds"
[GIN] 2025/04/08 - 22:41:33 | 200 | 51.813764087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/08 - 22:44:45 | 200 |  7.807525698s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/13 - 14:39:17 | 200 |    6.059991ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/13 - 14:39:17 | 200 |  147.106373ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-13T14:39:18.742+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-13T14:39:18.743+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-13T14:39:18.743+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-13T14:39:18.743+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-13T14:39:20.052+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="220.0 GiB" free_swap="39.6 MiB"
time=2025-04-13T14:39:20.052+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-13T14:39:20.052+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-13T14:39:20.052+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-13T14:39:20.052+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-13T14:39:20.395+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 42603"
time=2025-04-13T14:39:20.396+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-13T14:39:20.396+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-13T14:39:20.396+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-13T14:39:20.420+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-13T14:39:20.904+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-13T14:39:20.905+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:42603"
time=2025-04-13T14:39:21.152+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-13T14:40:08.863+08:00 level=INFO source=server.go:619 msg="llama runner started in 48.47 seconds"
[GIN] 2025/04/13 - 14:40:08 | 200 | 51.575562421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/13 - 14:40:58 | 200 | 10.873386796s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/13 - 14:41:42 | 200 |  6.107108657s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/13 - 14:42:59 | 200 |  1.356950833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/13 - 14:43:41 | 200 |  3.339527851s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/14 - 21:34:04 | 200 |      97.716µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/14 - 21:34:04 | 200 |   70.019028ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-14T21:34:05.748+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-14T21:34:05.748+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-14T21:34:05.748+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-14T21:34:05.748+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-14T21:34:06.816+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="241.8 GiB" free_swap="39.6 MiB"
time=2025-04-14T21:34:06.816+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-14T21:34:06.816+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-14T21:34:06.816+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-14T21:34:06.816+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-14T21:34:07.079+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 32905"
time=2025-04-14T21:34:07.080+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-14T21:34:07.080+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-14T21:34:07.080+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-14T21:34:07.106+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-14T21:34:07.195+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-14T21:34:07.195+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:32905"
time=2025-04-14T21:34:07.332+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-14T21:34:09.341+08:00 level=INFO source=server.go:619 msg="llama runner started in 2.26 seconds"
[GIN] 2025/04/14 - 21:34:09 | 200 |  4.912542863s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/14 - 21:36:36 | 200 |  1.110027226s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/14 - 21:37:07 | 200 |  1.593567807s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/14 - 21:41:00 | 200 |  1.471991742s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/14 - 21:42:17 | 200 |  2.110873208s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/15 - 12:43:48 | 200 |      75.429µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/15 - 12:43:48 | 200 |   69.266158ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-15T12:43:49.759+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-15T12:43:49.760+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-15T12:43:49.760+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-15T12:43:49.760+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-15T12:43:50.806+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="241.7 GiB" free_swap="39.6 MiB"
time=2025-04-15T12:43:50.806+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-15T12:43:50.807+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-15T12:43:50.807+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-15T12:43:50.807+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-15T12:43:51.065+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 34631"
time=2025-04-15T12:43:51.065+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-15T12:43:51.065+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-15T12:43:51.065+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-15T12:43:51.088+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-15T12:43:51.165+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-15T12:43:51.166+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:34631"
time=2025-04-15T12:43:51.317+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-15T12:43:53.327+08:00 level=INFO source=server.go:619 msg="llama runner started in 2.26 seconds"
[GIN] 2025/04/15 - 12:43:53 | 200 |  4.856463497s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/15 - 12:44:08 | 200 |  7.202104678s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/15 - 12:44:23 | 200 |  5.021822154s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/15 - 12:44:36 | 200 |  3.476849071s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/15 - 12:44:52 | 200 |      57.055µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/15 - 12:44:52 | 200 |   71.834642ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/15 - 12:44:52 | 200 |   40.155775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/15 - 12:45:01 | 200 |  3.065271454s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/15 - 12:45:09 | 200 |  2.687744943s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 15:20:28 | 200 |      76.895µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:21:22 | 201 |    15.651993s |       127.0.0.1 | POST     "/api/blobs/sha256:287ed68ae03da465f550db5dacda12d283efa5aed31dbe0f49f16399974495e6"
[GIN] 2025/04/18 - 15:21:38 | 201 | 15.614132011s |       127.0.0.1 | POST     "/api/blobs/sha256:e18aa20db0ddac23511e2ebde7573e3401352b37e410e3d632930ed5731ee81e"
[GIN] 2025/04/18 - 15:21:38 | 201 |     633.485µs |       127.0.0.1 | POST     "/api/blobs/sha256:11a319a91d25675435c7f5b0bee023f1b5b8b44290f876ecdd9ffc171cb3f51f"
[GIN] 2025/04/18 - 15:21:53 | 201 | 15.762788371s |       127.0.0.1 | POST     "/api/blobs/sha256:fd91d43f4b463fdbc3453de8353285ac9afb81458aa6c621fc764c62c287a169"
[GIN] 2025/04/18 - 15:21:58 | 201 |   4.10155355s |       127.0.0.1 | POST     "/api/blobs/sha256:f5d6e7a1fe72ea9b7ab98953ba296d7a5f6c0c462718d2c32518ac29cf794f34"
[GIN] 2025/04/18 - 15:21:58 | 201 |     692.819µs |       127.0.0.1 | POST     "/api/blobs/sha256:5296e6593fb4d31327def81e9255fbd0b782ba890b52e68f409c4488a322b34a"
[GIN] 2025/04/18 - 15:21:58 | 201 |    1.089584ms |       127.0.0.1 | POST     "/api/blobs/sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b"
[GIN] 2025/04/18 - 15:21:58 | 201 |     237.209µs |       127.0.0.1 | POST     "/api/blobs/sha256:b82a7fb78938eefe5da710322cf3cdb591d87c175c20d7bf852c27a36de2e472"
[GIN] 2025/04/18 - 15:21:58 | 201 |  105.484431ms |       127.0.0.1 | POST     "/api/blobs/sha256:6b9e4e7fb171f92fd137b777cc2714bf87d11576700a1dcd7a399e7bbe39537b"
[GIN] 2025/04/18 - 15:21:58 | 201 |    1.466784ms |       127.0.0.1 | POST     "/api/blobs/sha256:13cc2c56c24c603b0f1ecb359ca653d39e2972da07d6195ca91dafef26ddd57a"
[GIN] 2025/04/18 - 15:22:53 | 200 |     386.966µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:22:53 | 200 |   249.40268ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/04/18 - 15:23:07 | 200 |      66.461µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:23:07 | 200 |    1.278662ms |       127.0.0.1 | GET      "/api/tags"
time=2025-04-18T15:23:43.266+08:00 level=ERROR source=create.go:162 msg="error converting from safetensors" error="write /tmp/ollama-safetensors1276462721/fp163440627421: no space left on device"
[GIN] 2025/04/18 - 15:23:43 | 200 |         1m45s |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/04/18 - 15:27:27 | 200 |      63.747µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:28:06 | 200 |     1.78501ms |       127.0.0.1 | POST     "/api/blobs/sha256:5296e6593fb4d31327def81e9255fbd0b782ba890b52e68f409c4488a322b34a"
[GIN] 2025/04/18 - 15:28:06 | 200 |     181.386µs |       127.0.0.1 | POST     "/api/blobs/sha256:6b9e4e7fb171f92fd137b777cc2714bf87d11576700a1dcd7a399e7bbe39537b"
[GIN] 2025/04/18 - 15:28:06 | 200 |     225.309µs |       127.0.0.1 | POST     "/api/blobs/sha256:13cc2c56c24c603b0f1ecb359ca653d39e2972da07d6195ca91dafef26ddd57a"
[GIN] 2025/04/18 - 15:28:06 | 200 |     181.852µs |       127.0.0.1 | POST     "/api/blobs/sha256:287ed68ae03da465f550db5dacda12d283efa5aed31dbe0f49f16399974495e6"
[GIN] 2025/04/18 - 15:28:06 | 200 |     180.665µs |       127.0.0.1 | POST     "/api/blobs/sha256:f5d6e7a1fe72ea9b7ab98953ba296d7a5f6c0c462718d2c32518ac29cf794f34"
[GIN] 2025/04/18 - 15:28:06 | 200 |     193.043µs |       127.0.0.1 | POST     "/api/blobs/sha256:11a319a91d25675435c7f5b0bee023f1b5b8b44290f876ecdd9ffc171cb3f51f"
[GIN] 2025/04/18 - 15:28:06 | 200 |      157.82µs |       127.0.0.1 | POST     "/api/blobs/sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b"
[GIN] 2025/04/18 - 15:28:06 | 200 |      143.68µs |       127.0.0.1 | POST     "/api/blobs/sha256:b82a7fb78938eefe5da710322cf3cdb591d87c175c20d7bf852c27a36de2e472"
[GIN] 2025/04/18 - 15:28:06 | 200 |      120.11µs |       127.0.0.1 | POST     "/api/blobs/sha256:e18aa20db0ddac23511e2ebde7573e3401352b37e410e3d632930ed5731ee81e"
[GIN] 2025/04/18 - 15:28:06 | 200 |      86.726µs |       127.0.0.1 | POST     "/api/blobs/sha256:fd91d43f4b463fdbc3453de8353285ac9afb81458aa6c621fc764c62c287a169"
time=2025-04-18T15:29:54.410+08:00 level=ERROR source=create.go:162 msg="error converting from safetensors" error="write /tmp/ollama-safetensors2497101312/fp162396520577: no space left on device"
[GIN] 2025/04/18 - 15:29:54 | 200 |         1m47s |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/04/18 - 15:34:03 | 200 |      72.776µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:34:41 | 200 |     232.749µs |       127.0.0.1 | POST     "/api/blobs/sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b"
[GIN] 2025/04/18 - 15:34:41 | 200 |     128.529µs |       127.0.0.1 | POST     "/api/blobs/sha256:6b9e4e7fb171f92fd137b777cc2714bf87d11576700a1dcd7a399e7bbe39537b"
[GIN] 2025/04/18 - 15:34:41 | 200 |     219.406µs |       127.0.0.1 | POST     "/api/blobs/sha256:287ed68ae03da465f550db5dacda12d283efa5aed31dbe0f49f16399974495e6"
[GIN] 2025/04/18 - 15:34:41 | 200 |     208.963µs |       127.0.0.1 | POST     "/api/blobs/sha256:f5d6e7a1fe72ea9b7ab98953ba296d7a5f6c0c462718d2c32518ac29cf794f34"
[GIN] 2025/04/18 - 15:34:41 | 200 |     161.903µs |       127.0.0.1 | POST     "/api/blobs/sha256:b82a7fb78938eefe5da710322cf3cdb591d87c175c20d7bf852c27a36de2e472"
[GIN] 2025/04/18 - 15:34:41 | 200 |      153.96µs |       127.0.0.1 | POST     "/api/blobs/sha256:13cc2c56c24c603b0f1ecb359ca653d39e2972da07d6195ca91dafef26ddd57a"
[GIN] 2025/04/18 - 15:34:41 | 200 |     120.199µs |       127.0.0.1 | POST     "/api/blobs/sha256:e18aa20db0ddac23511e2ebde7573e3401352b37e410e3d632930ed5731ee81e"
[GIN] 2025/04/18 - 15:34:41 | 200 |     158.319µs |       127.0.0.1 | POST     "/api/blobs/sha256:fd91d43f4b463fdbc3453de8353285ac9afb81458aa6c621fc764c62c287a169"
[GIN] 2025/04/18 - 15:34:41 | 200 |     156.287µs |       127.0.0.1 | POST     "/api/blobs/sha256:5296e6593fb4d31327def81e9255fbd0b782ba890b52e68f409c4488a322b34a"
[GIN] 2025/04/18 - 15:34:41 | 200 |     120.714µs |       127.0.0.1 | POST     "/api/blobs/sha256:11a319a91d25675435c7f5b0bee023f1b5b8b44290f876ecdd9ffc171cb3f51f"
time=2025-04-18T15:36:28.837+08:00 level=ERROR source=create.go:162 msg="error converting from safetensors" error="write /tmp/ollama-safetensors1714104721/fp16463759614: no space left on device"
[GIN] 2025/04/18 - 15:36:28 | 200 |         1m47s |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/04/18 - 15:38:25 | 200 |      80.627µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:38:25 | 200 |  121.582432ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/18 - 15:38:45 | 200 |      68.079µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:38:45 | 200 |    1.863695ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 15:38:46 | 200 |  1.000264807s |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/04/18 - 15:48:18 | 200 |      71.183µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:48:57 | 200 |    1.901002ms |       127.0.0.1 | POST     "/api/blobs/sha256:13cc2c56c24c603b0f1ecb359ca653d39e2972da07d6195ca91dafef26ddd57a"
[GIN] 2025/04/18 - 15:48:57 | 200 |      143.71µs |       127.0.0.1 | POST     "/api/blobs/sha256:287ed68ae03da465f550db5dacda12d283efa5aed31dbe0f49f16399974495e6"
[GIN] 2025/04/18 - 15:48:57 | 200 |     165.458µs |       127.0.0.1 | POST     "/api/blobs/sha256:e18aa20db0ddac23511e2ebde7573e3401352b37e410e3d632930ed5731ee81e"
[GIN] 2025/04/18 - 15:48:57 | 200 |     213.732µs |       127.0.0.1 | POST     "/api/blobs/sha256:fd91d43f4b463fdbc3453de8353285ac9afb81458aa6c621fc764c62c287a169"
[GIN] 2025/04/18 - 15:48:57 | 200 |     146.175µs |       127.0.0.1 | POST     "/api/blobs/sha256:f5d6e7a1fe72ea9b7ab98953ba296d7a5f6c0c462718d2c32518ac29cf794f34"
[GIN] 2025/04/18 - 15:48:57 | 200 |     177.076µs |       127.0.0.1 | POST     "/api/blobs/sha256:5296e6593fb4d31327def81e9255fbd0b782ba890b52e68f409c4488a322b34a"
[GIN] 2025/04/18 - 15:48:57 | 200 |     155.565µs |       127.0.0.1 | POST     "/api/blobs/sha256:6b9e4e7fb171f92fd137b777cc2714bf87d11576700a1dcd7a399e7bbe39537b"
[GIN] 2025/04/18 - 15:48:57 | 200 |      170.63µs |       127.0.0.1 | POST     "/api/blobs/sha256:11a319a91d25675435c7f5b0bee023f1b5b8b44290f876ecdd9ffc171cb3f51f"
[GIN] 2025/04/18 - 15:48:57 | 200 |     153.554µs |       127.0.0.1 | POST     "/api/blobs/sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b"
[GIN] 2025/04/18 - 15:48:57 | 200 |     106.892µs |       127.0.0.1 | POST     "/api/blobs/sha256:b82a7fb78938eefe5da710322cf3cdb591d87c175c20d7bf852c27a36de2e472"
time=2025-04-18T15:50:44.085+08:00 level=ERROR source=create.go:162 msg="error converting from safetensors" error="write /tmp/ollama-safetensors4206952522/fp162629310058: no space left on device"
[GIN] 2025/04/18 - 15:50:44 | 200 |         1m46s |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/04/18 - 15:53:40 | 200 |      78.212µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 15:53:40 | 200 |      68.043µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/04/18 - 16:31:21 | 200 |      79.835µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:31:21 | 200 |     885.136µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/04/18 - 16:33:46 | 200 |      66.243µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:33:46 | 200 |   70.631715ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/18 - 16:34:10 | 200 |      84.195µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:34:10 | 200 |   77.603692ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/18 - 16:35:47 | 200 |      63.341µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:37:22 | 201 | 54.911955847s |       127.0.0.1 | POST     "/api/blobs/sha256:9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca"
[GIN] 2025/04/18 - 16:37:22 | 200 |  183.653673ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/04/18 - 16:38:35 | 200 |      74.109µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:38:35 | 200 |   69.479351ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-18T16:38:36.673+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T16:38:36.676+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-18T16:38:37.751+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="237.9 GiB" free_swap="0 B"
time=2025-04-18T16:38:37.751+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T16:38:37.751+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Recall
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Recall
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-18T16:38:38.029+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 43223"
time=2025-04-18T16:38:38.029+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-18T16:38:38.029+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-18T16:38:38.029+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-18T16:38:38.055+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-18T16:38:38.143+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-18T16:38:38.143+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:43223"
time=2025-04-18T16:38:38.282+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Recall
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Recall
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-18T16:38:45.314+08:00 level=INFO source=server.go:619 msg="llama runner started in 7.28 seconds"
[GIN] 2025/04/18 - 16:38:45 | 200 |  9.933329377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:39:42 | 200 |  2.173226287s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:39:55 | 200 |  912.962855ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:40:06 | 200 |  1.335639588s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:40:17 | 200 |   1.22925537s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:40:32 | 200 |  1.320344721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:40:53 | 200 |  1.127187104s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:41:03 | 200 |  622.677364ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:41:12 | 200 |  1.117383433s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:41:22 | 200 |  470.690196ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:41:38 | 200 |      83.816µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:41:38 | 200 |       67.52µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/04/18 - 16:42:37 | 200 |  1.188930036s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:43:51 | 200 |      60.956µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:45:24 | 201 | 53.630913024s |       127.0.0.1 | POST     "/api/blobs/sha256:f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791"
[GIN] 2025/04/18 - 16:45:24 | 200 |  290.420484ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/04/18 - 16:45:58 | 200 |     117.612µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:45:58 | 404 |    1.553977ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/18 - 16:46:00 | 200 |  2.098489855s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/04/18 - 16:46:03 | 200 |       47.96µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:46:03 | 200 |   70.253044ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 library=cuda total="23.7 GiB" available="7.1 GiB"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-a12a891f-f7a9-80bb-b7be-75023d0b09f2 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-65c85600-e089-d091-edab-738b9e487c22 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-7216d8b7-5bd2-ce19-3833-a353d4261825 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-09c0641d-f776-bbe5-6211-31b61fd2ef8b library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-141da65b-4c7a-fdf4-8f23-cea937e4d38c library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-55c12161-e849-4ab9-1f7b-9470821e7df4 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T16:46:04.800+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T16:46:04.800+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 gpu=GPU-a12a891f-f7a9-80bb-b7be-75023d0b09f2 parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-18T16:46:05.775+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="236.7 GiB" free_swap="0 B"
time=2025-04-18T16:46:05.776+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T16:46:05.776+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Cot
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Cot
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-18T16:46:06.031+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 35597"
time=2025-04-18T16:46:06.031+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=2
time=2025-04-18T16:46:06.031+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-18T16:46:06.031+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-18T16:46:06.058+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-18T16:46:06.147+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-18T16:46:06.147+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:35597"
time=2025-04-18T16:46:06.284+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Cot
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Cot
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-18T16:46:11.810+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.78 seconds"
[GIN] 2025/04/18 - 16:46:11 | 200 |  8.203144358s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:46:58 | 200 |  1.258901894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:46:59 | 200 |  733.018257ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:49:49 | 200 |  1.439321453s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:49:50 | 200 |  705.916981ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:50:29 | 200 |      62.912µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:50:29 | 200 |   68.574508ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/18 - 16:50:29 | 200 |   39.523796ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:50:43 | 200 |  8.585769537s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:51:26 | 200 | 10.296020822s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:51:29 | 200 |      58.239µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/18 - 16:51:29 | 200 |   67.568087ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/18 - 16:51:29 | 200 |   43.700273ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:51:51 | 200 | 11.861258943s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/18 - 16:53:45 | 200 |  1.142181337s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:53:46 | 200 |  1.146135539s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:54:40 | 200 |  1.211617613s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:54:41 | 200 |  700.930717ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:55:27 | 200 |    1.4656423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:55:39 | 200 | 11.492653435s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:55:50 | 200 |   499.12867ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:56:00 | 200 | 10.395333451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:56:20 | 200 |  553.055298ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:56:32 | 200 | 11.553488315s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:56:49 | 200 |  1.435079652s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/18 - 16:57:00 | 200 |  11.47795449s |       127.0.0.1 | POST     "/api/generate"
time=2025-04-18T20:02:25.135+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T20:02:25.136+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-18T20:02:26.021+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="240.1 GiB" free_swap="0 B"
time=2025-04-18T20:02:26.021+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T20:02:26.021+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Recall
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Recall
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-18T20:02:26.269+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 33853"
time=2025-04-18T20:02:26.270+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-18T20:02:26.270+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-18T20:02:26.270+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-18T20:02:26.295+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-18T20:02:26.381+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-18T20:02:26.382+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:33853"
time=2025-04-18T20:02:26.523+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-9be954355f946d93594b88361424bcaf53199fbdc34a0fc49b09f09b0c018aca (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Recall
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Recall
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-18T20:02:31.797+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.53 seconds"
[GIN] 2025/04/18 - 20:02:33 | 200 |  9.082446365s |       127.0.0.1 | POST     "/api/generate"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 library=cuda total="23.7 GiB" available="7.1 GiB"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-a12a891f-f7a9-80bb-b7be-75023d0b09f2 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-65c85600-e089-d091-edab-738b9e487c22 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-7216d8b7-5bd2-ce19-3833-a353d4261825 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-09c0641d-f776-bbe5-6211-31b61fd2ef8b library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-141da65b-4c7a-fdf4-8f23-cea937e4d38c library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-ab352d65-a920-a501-b936-bbaf0da12d4d library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T20:02:34.285+08:00 level=INFO source=sched.go:509 msg="updated VRAM based on existing loaded models" gpu=GPU-55c12161-e849-4ab9-1f7b-9470821e7df4 library=cuda total="23.7 GiB" available="23.5 GiB"
time=2025-04-18T20:02:34.285+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T20:02:34.286+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 gpu=GPU-a12a891f-f7a9-80bb-b7be-75023d0b09f2 parallel=4 available=25180176384 required="16.4 GiB"
time=2025-04-18T20:02:35.265+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="239.0 GiB" free_swap="0 B"
time=2025-04-18T20:02:35.265+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-18T20:02:35.266+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="16.4 GiB" memory.required.partial="16.4 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[16.4 GiB]" memory.weights.total="14.0 GiB" memory.weights.repeating="13.0 GiB" memory.weights.nonrepeating="1002.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Cot
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Cot
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-18T20:02:35.526+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 44193"
time=2025-04-18T20:02:35.527+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=2
time=2025-04-18T20:02:35.527+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-18T20:02:35.527+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-18T20:02:35.554+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-18T20:02:35.643+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-18T20:02:35.644+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:44193"
time=2025-04-18T20:02:35.783+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-f767f2ec1180b330f5d7b397d2ba9f6ba53d5246094625bbeed4503ae5784791 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 刘星_Cot
llama_model_loader: - kv   3:                         general.size_label str              = 8.0B
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                       llama.context_length u32              = 131072
llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128009
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type bf16:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = BF16
print_info: file size   = 14.96 GiB (16.00 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = 刘星_Cot
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size = 14315.02 MiB
load_tensors:   CPU_Mapped model buffer size =  1002.00 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-18T20:02:41.057+08:00 level=INFO source=server.go:619 msg="llama runner started in 5.53 seconds"
[GIN] 2025/04/18 - 20:02:56 | 200 | 23.279998367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/21 - 22:13:59 | 200 |      89.298µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/21 - 22:13:59 | 200 |   71.495797ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-21T22:14:00.393+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-21T22:14:00.394+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-21T22:14:00.394+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-21T22:14:00.394+08:00 level=INFO source=sched.go:716 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-d9018970-f522-8525-fedc-8e7d5378e0f3 parallel=4 available=25180176384 required="6.5 GiB"
time=2025-04-21T22:14:01.445+08:00 level=INFO source=server.go:105 msg="system memory" total="251.6 GiB" free="240.2 GiB" free_swap="45.8 MiB"
time=2025-04-21T22:14:01.445+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.vision.block_count default=0
time=2025-04-21T22:14:01.445+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.key_length default=128
time=2025-04-21T22:14:01.445+08:00 level=WARN source=ggml.go:149 msg="key not found" key=llama.attention.value_length default=128
time=2025-04-21T22:14:01.445+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-04-21T22:14:01.708+08:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/data/hfc/ollama/bin/ollama runner --model /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 40 --parallel 4 --port 36671"
time=2025-04-21T22:14:01.708+08:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-21T22:14:01.708+08:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-21T22:14:01.709+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-21T22:14:01.735+08:00 level=INFO source=runner.go:858 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /data/hfc/ollama/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /data/hfc/ollama/lib/ollama/libggml-cpu-skylakex.so
time=2025-04-21T22:14:01.984+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-04-21T22:14:01.984+08:00 level=INFO source=runner.go:918 msg="Server listening on 127.0.0.1:36671"
time=2025-04-21T22:14:02.212+08:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23915 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/hfc/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     2.02 MiB
llama_init_from_model:      CUDA0 compute buffer size =   560.00 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    24.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 2
time=2025-04-21T22:14:04.222+08:00 level=INFO source=server.go:619 msg="llama runner started in 2.51 seconds"
[GIN] 2025/04/21 - 22:14:04 | 200 |  5.135214921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/21 - 22:14:23 | 200 |   1.64755734s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/21 - 22:15:00 | 200 |  1.176555755s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/21 - 22:15:15 | 200 |  147.686801ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/21 - 22:15:25 | 200 |  251.859267ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/21 - 22:15:35 | 200 |      68.522µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/21 - 22:15:35 | 200 |   71.984955ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/21 - 22:15:35 | 200 |   43.723494ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/04/21 - 22:15:44 | 200 |    2.2254957s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/21 - 22:16:04 | 200 |   1.38667697s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/21 - 22:16:57 | 200 |  1.202289296s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/21 - 22:17:07 | 200 |  178.378271ms |       127.0.0.1 | POST     "/api/chat"
